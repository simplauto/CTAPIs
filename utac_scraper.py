#!/usr/bin/env python3
import requests
from bs4 import BeautifulSoup
import urllib3
from urllib.parse import urljoin

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class UTACScraper:
    def __init__(self):
        self.base_url = "https://www.utac-otc.com"
        self.session = requests.Session()
        self.session.verify = False
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def get_page(self, url):
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except requests.RequestException as e:
            print(f"Erreur lors de la r√©cup√©ration de la page: {e}")
            return None
    
    def scrape_ct_search_page(self):
        url = "https://www.utac-otc.com/vehicule_leger/Pages/Retrouver_un_CT.aspx"
        soup = self.get_page(url)
        
        if not soup:
            return None
        
        result = {
            'page_title': soup.title.text.strip() if soup.title else '',
            'forms': [],
            'input_fields': [],
            'tables': [],
            'links': []
        }
        
        # Chercher les formulaires
        forms = soup.find_all('form')
        for form in forms:
            form_data = {
                'id': form.get('id', ''),
                'action': form.get('action', ''),
                'method': form.get('method', 'GET'),
                'inputs': []
            }
            
            inputs = form.find_all(['input', 'select', 'textarea'])
            for inp in inputs:
                input_data = {
                    'name': inp.get('name', ''),
                    'type': inp.get('type', ''),
                    'id': inp.get('id', ''),
                    'placeholder': inp.get('placeholder', ''),
                    'value': inp.get('value', '')
                }
                form_data['inputs'].append(input_data)
            
            result['forms'].append(form_data)
        
        # Chercher tous les champs input
        inputs = soup.find_all(['input', 'select', 'textarea'])
        for inp in inputs:
            input_data = {
                'name': inp.get('name', ''),
                'type': inp.get('type', ''),
                'id': inp.get('id', ''),
                'placeholder': inp.get('placeholder', ''),
                'value': inp.get('value', '')
            }
            result['input_fields'].append(input_data)
        
        # Chercher les tableaux
        tables = soup.find_all('table')
        for i, table in enumerate(tables):
            headers = [th.get_text().strip() for th in table.find_all('th')]
            rows = []
            for tr in table.find_all('tr')[1:]:  # Skip header row
                row = [td.get_text().strip() for td in tr.find_all('td')]
                if row:
                    rows.append(row)
            
            result['tables'].append({
                'index': i,
                'headers': headers,
                'rows': rows[:5]  # Limit to first 5 rows
            })
        
        # Chercher les liens importants
        links = soup.find_all('a', href=True)
        for link in links:
            href = link.get('href')
            if href and not href.startswith('#'):
                full_url = urljoin(url, href)
                result['links'].append({
                    'text': link.get_text().strip()[:100],
                    'href': full_url
                })
        
        return result
    
    def search_by_agreement_number(self, agreement_number):
        """
        Recherche un centre de contr√¥le technique par num√©ro d'agr√©ment
        """
        url = "https://www.utac-otc.com/vehicule_leger/Pages/Retrouver_un_CT.aspx"
        soup = self.get_page(url)
        
        if not soup:
            return None
        
        # Trouver le formulaire principal
        form = soup.find('form', {'id': 'aspnetForm'})
        if not form:
            print("Formulaire principal non trouv√©")
            return None
        
        # R√©cup√©rer tous les champs cach√©s n√©cessaires pour ASP.NET
        form_data = {}
        hidden_inputs = form.find_all('input', {'type': 'hidden'})
        for inp in hidden_inputs:
            name = inp.get('name')
            value = inp.get('value', '')
            if name:
                form_data[name] = value
        
        # Trouver le champ de crit√®re et le dropdown
        criteria_field = None
        dropdown_field = None
        
        # Rechercher tous les inputs et selects
        all_inputs = form.find_all(['input', 'select'])
        for inp in all_inputs:
            inp_name = inp.get('name', '')
            
            # Chercher le champ crit√®re (input text qui contient "critereValueInput")
            if inp.name == 'input' and inp.get('type') == 'text' and 'critereValueInput' in inp_name:
                criteria_field = inp_name
            
            # Chercher le dropdown qui contient "ddlCritereField"
            if inp.name == 'select' and 'ddlCritereField' in inp_name:
                dropdown_field = inp_name
                # S√©lectionner l'option "Agrement"
                form_data[dropdown_field] = 'Agrement'
        
        if not criteria_field or not dropdown_field:
            print("Champs de recherche non trouv√©s")
            return self._debug_form_fields(form)
        
        # Remplir le formulaire
        form_data[criteria_field] = agreement_number
        
        # Trouver et ajouter le bouton de recherche
        search_button = form.find('input', {'type': 'submit', 'value': 'Rechercher'})
        if search_button:
            button_name = search_button.get('name')
            if button_name:
                form_data[button_name] = search_button.get('value', 'Rechercher')
        
        # Ajouter les donn√©es de soumission ASP.NET
        form_data['__EVENTTARGET'] = ''
        form_data['__EVENTARGUMENT'] = ''
        
        # Envoyer la requ√™te POST
        try:
            response = self.session.post(url, data=form_data, timeout=10)
            response.raise_for_status()
            
            # Parser la r√©ponse
            result_soup = BeautifulSoup(response.content, 'html.parser')
            return self._parse_search_results(result_soup, agreement_number)
            
        except requests.RequestException as e:
            print(f"Erreur lors de la recherche: {e}")
            return None
    
    def _debug_form_fields(self, form):
        """Fonction de debug pour afficher tous les champs du formulaire"""
        print("=== DEBUG: Champs du formulaire ===")
        all_inputs = form.find_all(['input', 'select', 'textarea'])
        for inp in all_inputs:
            print(f"Tag: {inp.name}, Name: {inp.get('name', 'N/A')}, ID: {inp.get('id', 'N/A')}, Type: {inp.get('type', 'N/A')}")
            if inp.name == 'select':
                options = inp.find_all('option')
                for opt in options:
                    print(f"  Option: {opt.get('value', 'N/A')} - {opt.get_text().strip()}")
        return {"debug": "Champs du formulaire affich√©s"}
    
    def _parse_search_results(self, soup, agreement_number):
        """Parse les r√©sultats de recherche et trouve les informations"""
        
        # D'abord, regarder si on a des informations directement dans les r√©sultats
        result_info = self._extract_info_from_results_page(soup, agreement_number)
        if result_info and any(result_info[key] for key in ['raison_sociale', 'enseigne', 'adresse', 'ville', 'telephone']):
            return result_info
        
        # Chercher le tableau de r√©sultats et les liens JavaScript
        tables = soup.find_all('table')
        detail_link = None
        
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                for cell in cells:
                    # Chercher le lien "Voir le d√©tail"
                    link = cell.find('a', string=lambda text: text and 'd√©tail' in text.lower())
                    if link:
                        href = link.get('href')
                        if href and href.startswith('javascript:'):
                            # Extraire les param√®tres du JavaScript
                            detail_link = self._handle_javascript_link(href, soup)
                        else:
                            detail_link = href
                        break
                if detail_link:
                    break
            if detail_link:
                break
        
        if not detail_link:
            print("Lien 'Voir le d√©tail' non trouv√©")
            # Afficher plus d'info de debug
            tables_info = []
            for i, table in enumerate(tables):
                rows = table.find_all('tr')
                if rows:
                    tables_info.append(f"Table {i}: {len(rows)} rows")
                    for j, row in enumerate(rows[:2]):  # First 2 rows
                        cells = [cell.get_text().strip() for cell in row.find_all(['td', 'th'])]
                        tables_info.append(f"  Row {j}: {cells}")
            
            return {"error": "Lien d√©tail non trouv√©", "debug_tables": tables_info}
        
        # Construire l'URL compl√®te du lien d√©tail
        if detail_link.startswith('http'):
            detail_url = detail_link
        else:
            detail_url = urljoin("https://www.utac-otc.com/vehicule_leger/Pages/", detail_link)
        
        # R√©cup√©rer la page de d√©tail
        return self._get_agreement_details(detail_url, agreement_number)
    
    def _extract_info_from_results_page(self, soup, agreement_number):
        """Essaie d'extraire les informations directement de la page de r√©sultats"""
        details = {
            'agreement_number': agreement_number,
            'raison_sociale': '',
            'enseigne': '',
            'adresse': '',
            'ville': '',
            'telephone': '',
            'option': '',
            'site_internet': '',
            'url': 'Page de r√©sultats'
        }
        
        # Chercher dans tous les tableaux
        tables = soup.find_all('table')
        for table in tables:
            # D'abord, analyser les en-t√™tes pour comprendre la structure
            headers = table.find_all('th')
            header_mapping = {}
            
            if headers:
                for i, th in enumerate(headers):
                    header_text = th.get_text().strip().lower()
                    if 'raison sociale' in header_text:
                        header_mapping['raison_sociale'] = i
                    elif 'agrement' in header_text:
                        header_mapping['agreement_number'] = i
                    elif 'enseigne' in header_text:
                        header_mapping['enseigne'] = i
                    elif 'adresse' in header_text:
                        header_mapping['adresse'] = i
                    elif 'ville' in header_text:
                        header_mapping['ville'] = i
                    elif 't√©l' in header_text:
                        header_mapping['telephone'] = i
                    elif 'option' in header_text:
                        header_mapping['option'] = i
                    elif 'site internet' in header_text:
                        header_mapping['site_internet'] = i
            
            # Chercher la ligne avec le num√©ro d'agr√©ment
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                row_text = ' '.join([cell.get_text().strip() for cell in cells])
                
                # Si la ligne contient le num√©ro d'agr√©ment, extraire toutes les informations
                if agreement_number in row_text:
                    # Mapping bas√© sur la structure connue du tableau UTAC-OTC:
                    # [0] Raison sociale, [1] Agr√©ment, [2] Enseigne (hidden), [3] Adresse
                    # [4] Ville, [5] T√©l (hidden), [6] Option, [7] Site internet (hidden), [8] D√©tails
                    
                    if len(cells) >= 8:
                        details['raison_sociale'] = cells[0].get_text().strip()
                        details['agreement_number'] = cells[1].get_text().strip()
                        details['enseigne'] = cells[2].get_text().strip()  # Colonne cach√©e
                        details['adresse'] = cells[3].get_text().strip()
                        details['ville'] = cells[4].get_text().strip()
                        details['telephone'] = cells[5].get_text().strip()  # Colonne cach√©e
                        details['option'] = cells[6].get_text().strip()
                        details['site_internet'] = cells[7].get_text().strip()  # Colonne cach√©e
                    
                    # Alternative : utiliser le mapping des en-t√™tes si disponible
                    elif header_mapping:
                        for field, index in header_mapping.items():
                            if index < len(cells):
                                details[field] = cells[index].get_text().strip()
                    
                    break  # On a trouv√© la ligne, pas besoin de continuer
        
        return details
    
    def _handle_javascript_link(self, js_link, soup):
        """G√®re les liens JavaScript ASP.NET"""
        # Pour les liens JavaScript ASP.NET, nous devons simuler le postback
        # Pour l'instant, retournons None pour √©viter l'erreur
        print(f"Lien JavaScript d√©tect√©: {js_link}")
        return None
    
    def _get_agreement_details(self, detail_url, agreement_number):
        """R√©cup√®re les d√©tails du centre d'agr√©ment"""
        
        detail_soup = self.get_page(detail_url)
        if not detail_soup:
            return {"error": "Impossible de r√©cup√©rer la page de d√©tail"}
        
        details = {
            'agreement_number': agreement_number,
            'enseigne': '',
            'adresse': '',
            'ville': '',
            'telephone': '',
            'url': detail_url
        }
        
        # Rechercher les informations dans le contenu de la page
        text_content = detail_soup.get_text()
        lines = [line.strip() for line in text_content.split('\n') if line.strip()]
        
        # Chercher les informations sp√©cifiques
        for i, line in enumerate(lines):
            line_lower = line.lower()
            
            if 'enseigne' in line_lower and ':' in line:
                details['enseigne'] = line.split(':', 1)[1].strip()
            elif 'adresse' in line_lower and ':' in line:
                details['adresse'] = line.split(':', 1)[1].strip()
            elif 'ville' in line_lower and ':' in line:
                details['ville'] = line.split(':', 1)[1].strip()
            elif 't√©l' in line_lower and ':' in line:
                details['telephone'] = line.split(':', 1)[1].strip()
        
        # M√©thode alternative: chercher dans les √©l√©ments structur√©s
        if not any(details[key] for key in ['enseigne', 'adresse', 'ville', 'telephone']):
            # Chercher dans les √©l√©ments avec des labels
            labels = detail_soup.find_all(['label', 'span', 'div', 'td'])
            for label in labels:
                text = label.get_text().strip()
                if ':' in text:
                    key, value = text.split(':', 1)
                    key = key.strip().lower()
                    value = value.strip()
                    
                    if 'enseigne' in key:
                        details['enseigne'] = value
                    elif 'adresse' in key:
                        details['adresse'] = value
                    elif 'ville' in key:
                        details['ville'] = value
                    elif 't√©l' in key:
                        details['telephone'] = value
        
        return details
    
    def search_by_department(self, department_code):
        """
        Recherche tous les centres de contr√¥le technique d'un d√©partement
        
        Args:
            department_code (str): Code du d√©partement (ex: "04", "75", "971")
            
        Returns:
            list: Liste des centres trouv√©s avec toutes leurs informations
        """
        # Valider le format du d√©partement
        if not self._validate_department_code(department_code):
            return {"error": f"Code d√©partement invalide: {department_code}"}
        
        url = "https://www.utac-otc.com/vehicule_leger/Pages/Retrouver_un_CT.aspx"
        soup = self.get_page(url)
        
        if not soup:
            return {"error": "Impossible de r√©cup√©rer la page de recherche"}
        
        # Trouver le formulaire principal
        form = soup.find('form', {'id': 'aspnetForm'})
        if not form:
            return {"error": "Formulaire principal non trouv√©"}
        
        # Pr√©parer les donn√©es du formulaire
        form_data = {}
        hidden_inputs = form.find_all('input', {'type': 'hidden'})
        for inp in hidden_inputs:
            name = inp.get('name')
            value = inp.get('value', '')
            if name:
                form_data[name] = value
        
        # Trouver les champs de recherche
        criteria_field = None
        dropdown_field = None
        
        all_inputs = form.find_all(['input', 'select'])
        for inp in all_inputs:
            inp_name = inp.get('name', '')
            
            if inp.name == 'input' and inp.get('type') == 'text' and 'critereValueInput' in inp_name:
                criteria_field = inp_name
            
            if inp.name == 'select' and 'ddlCritereField' in inp_name:
                dropdown_field = inp_name
        
        if not criteria_field or not dropdown_field:
            return {"error": "Champs de recherche non trouv√©s"}
        
        # Remplir le formulaire pour recherche par d√©partement
        form_data[criteria_field] = department_code
        form_data[dropdown_field] = 'Departement'
        
        # Ajouter le bouton de recherche
        search_button = form.find('input', {'type': 'submit', 'value': 'Rechercher'})
        if search_button:
            button_name = search_button.get('name')
            if button_name:
                form_data[button_name] = search_button.get('value', 'Rechercher')
        
        form_data['__EVENTTARGET'] = ''
        form_data['__EVENTARGUMENT'] = ''
        
        # Effectuer la recherche initiale
        try:
            response = self.session.post(url, data=form_data, timeout=10)
            response.raise_for_status()
            
            result_soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extraire tous les r√©sultats avec pagination
            all_centers = []
            page_number = 1
            
            while True:
                print(f"Traitement de la page {page_number}...")
                
                # Extraire les centres de la page actuelle
                page_centers = self._extract_all_centers_from_page(result_soup, department_code)
                all_centers.extend(page_centers)
                
                # Chercher le lien vers la page suivante
                next_page_link = self._find_next_page_link(result_soup)
                
                if not next_page_link:
                    print(f"Pas de page suivante trouv√©e. Total: {len(all_centers)} centres")
                    break
                
                # Naviguer vers la page suivante
                result_soup = self._navigate_to_next_page(result_soup, next_page_link)
                if not result_soup:
                    print("Erreur lors de la navigation vers la page suivante")
                    break
                
                page_number += 1
                
                # S√©curit√© : √©viter les boucles infinies
                if page_number > 50:
                    print("Limite de 50 pages atteinte, arr√™t de la pagination")
                    break
            
            return {
                "department_code": department_code,
                "total_centers": len(all_centers),
                "centers": all_centers
            }
            
        except requests.RequestException as e:
            return {"error": f"Erreur lors de la recherche: {e}"}
    
    def _validate_department_code(self, code):
        """Valide le format du code d√©partement"""
        if not code or not isinstance(code, str):
            return False
        
        # Supprimer les espaces
        code = code.strip()
        
        # V√©rifier le format
        if code.isdigit():
            num = int(code)
            # D√©partements 01-95 ou DOM-TOM 971-989
            return (1 <= num <= 95) or (971 <= num <= 989)
        
        return False
    
    def _extract_all_centers_from_page(self, soup, department_code):
        """Extrait tous les centres de la page actuelle"""
        centers = []
        
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 8:  # Ligne compl√®te de donn√©es
                    row_text = ' '.join([cell.get_text().strip() for cell in cells])
                    
                    # V√©rifier si cette ligne appartient au d√©partement recherch√©
                    if self._is_department_row(row_text, department_code):
                        center_info = self._extract_center_from_row(cells)
                        if center_info:
                            centers.append(center_info)
        
        return centers
    
    def _is_department_row(self, row_text, department_code):
        """V√©rifie si une ligne appartient au d√©partement recherch√©"""
        # Chercher les codes postaux correspondant au d√©partement
        dept_num = int(department_code)
        
        if dept_num <= 95:
            # D√©partements m√©tropolitains : codes postaux commencent par le num√©ro du d√©partement
            dept_formatted = f"{dept_num:02d}"  # Format 01, 02, etc.
            postal_codes = [f"{dept_formatted}{i:03d}" for i in range(1000)]  # 01000-01999, etc.
            return any(postal in row_text for postal in [dept_formatted + '0', dept_formatted + '1', dept_formatted + '2', dept_formatted + '3', dept_formatted + '4', dept_formatted + '5', dept_formatted + '6', dept_formatted + '7', dept_formatted + '8', dept_formatted + '9'])
        else:
            # DOM-TOM : codes postaux = num√©ro d√©partement
            return department_code in row_text
    
    def _extract_center_from_row(self, cells):
        """Extrait les informations d'un centre depuis une ligne de tableau"""
        if len(cells) < 8:
            return None
        
        return {
            'raison_sociale': cells[0].get_text().strip(),
            'agreement_number': cells[1].get_text().strip(),
            'enseigne': cells[2].get_text().strip(),
            'adresse': cells[3].get_text().strip(),
            'ville': cells[4].get_text().strip(),
            'telephone': cells[5].get_text().strip(),
            'option': cells[6].get_text().strip(),
            'site_internet': cells[7].get_text().strip() if len(cells) > 7 else ''
        }
    
    def _find_next_page_link(self, soup):
        """Trouve le lien vers la page suivante"""
        # Chercher les liens de pagination
        pagination_links = soup.find_all('a', href=lambda href: href and '__doPostBack' in href and 'Page$' in href)
        
        if not pagination_links:
            return None
        
        # Trouver le num√©ro de page actuelle
        current_page = 1
        page_spans = soup.find_all('span')
        for span in page_spans:
            text = span.get_text().strip()
            if text.isdigit():
                # V√©rifier si c'est la page courante (pas un lien)
                parent = span.parent
                if parent and parent.name != 'a':
                    current_page = int(text)
                    break
        
        # Chercher le lien vers la page suivante
        next_page = current_page + 1
        for link in pagination_links:
            href = link.get('href', '')
            if f'Page${next_page}' in href:
                return link
        
        return None
    
    def _navigate_to_next_page(self, current_soup, next_link):
        """Navigue vers la page suivante en simulant le postback ASP.NET"""
        href = next_link.get('href', '')
        
        # Extraire les param√®tres du JavaScript __doPostBack
        if '__doPostBack' not in href:
            return None
        
        # Parser le JavaScript pour extraire les param√®tres
        # Format: javascript:__doPostBack('target','argument')
        import re
        match = re.search(r"__doPostBack\('([^']+)','([^']+)'\)", href)
        if not match:
            return None
        
        event_target = match.group(1)
        event_argument = match.group(2)
        
        # Pr√©parer les donn√©es du formulaire pour le postback
        form = current_soup.find('form', {'id': 'aspnetForm'})
        if not form:
            return None
        
        form_data = {}
        hidden_inputs = form.find_all('input', {'type': 'hidden'})
        for inp in hidden_inputs:
            name = inp.get('name')
            value = inp.get('value', '')
            if name:
                form_data[name] = value
        
        # Ajouter les param√®tres du postback
        form_data['__EVENTTARGET'] = event_target
        form_data['__EVENTARGUMENT'] = event_argument
        
        # Effectuer la requ√™te POST
        try:
            url = "https://www.utac-otc.com/vehicule_leger/Pages/Retrouver_un_CT.aspx"
            response = self.session.post(url, data=form_data, timeout=10)
            response.raise_for_status()
            
            return BeautifulSoup(response.content, 'html.parser')
        
        except requests.RequestException as e:
            print(f"Erreur lors de la navigation: {e}")
            return None
    
    def get_all_french_centers(self, output_dir="/tmp/utac_departments"):
        """
        R√©cup√®re tous les centres de contr√¥le technique de France
        Sauvegarde incr√©mentale par d√©partement pour √©viter les pertes
        
        Args:
            output_dir (str): R√©pertoire pour les fichiers temporaires
            
        Returns:
            dict: R√©sultats complets avec statistiques
        """
        import os
        import json
        import time
        
        # Cr√©er le r√©pertoire de sortie
        os.makedirs(output_dir, exist_ok=True)
        
        # Liste compl√®te des d√©partements fran√ßais
        departments = []
        
        # D√©partements m√©tropolitains (01-95)
        for i in range(1, 96):
            departments.append(f"{i:02d}")
        
        # D√©partements d'outre-mer (971-989)
        for i in range(971, 990):
            departments.append(str(i))
        
        print(f"=== R√âCUP√âRATION DE TOUS LES CENTRES DE FRANCE ===")
        print(f"D√©partements √† traiter: {len(departments)}")
        print(f"Estimation: ~6700 centres en ~8-10 minutes")
        print(f"Sauvegarde incr√©mentale dans: {output_dir}")
        
        all_centers = []
        department_stats = {}
        total_centers = 0
        errors = []
        start_time = time.time()
        
        for i, dept in enumerate(departments, 1):
            dept_start_time = time.time()
            print(f"\n[{i:3d}/{len(departments)}] Traitement d√©partement {dept}...")
            
            try:
                result = self.search_by_department(dept)
                
                if result and 'error' not in result:
                    centers = result.get('centers', [])
                    dept_total = len(centers)
                    total_centers += dept_total
                    
                    # Ajouter le code d√©partement √† chaque centre
                    for center in centers:
                        center['department'] = dept
                    
                    all_centers.extend(centers)
                    
                    # Statistiques du d√©partement
                    dept_duration = time.time() - dept_start_time
                    department_stats[dept] = {
                        'centers_count': dept_total,
                        'duration_seconds': round(dept_duration, 2),
                        'status': 'success'
                    }
                    
                    # Sauvegarde incr√©mentale
                    dept_file = os.path.join(output_dir, f"dept_{dept}.json")
                    with open(dept_file, 'w', encoding='utf-8') as f:
                        json.dump({
                            'department_code': dept,
                            'total_centers': dept_total,
                            'centers': centers,
                            'timestamp': time.time()
                        }, f, indent=2, ensure_ascii=False)
                    
                    print(f"    ‚úÖ {dept_total} centres r√©cup√©r√©s en {dept_duration:.1f}s")
                    
                else:
                    error_msg = result.get('error', 'Erreur inconnue') if result else 'Pas de r√©sultat'
                    errors.append(f"D√©partement {dept}: {error_msg}")
                    department_stats[dept] = {
                        'centers_count': 0,
                        'duration_seconds': round(time.time() - dept_start_time, 2),
                        'status': 'error',
                        'error': error_msg
                    }
                    print(f"    ‚ùå Erreur: {error_msg}")
                
            except Exception as e:
                error_msg = str(e)
                errors.append(f"D√©partement {dept}: Exception - {error_msg}")
                department_stats[dept] = {
                    'centers_count': 0,
                    'duration_seconds': round(time.time() - dept_start_time, 2),
                    'status': 'exception',
                    'error': error_msg
                }
                print(f"    üí• Exception: {error_msg}")
            
            # Affichage du progr√®s
            elapsed = time.time() - start_time
            if i > 0:
                avg_time_per_dept = elapsed / i
                remaining_time = avg_time_per_dept * (len(departments) - i)
                print(f"    üìä Total: {total_centers} centres | Temps √©coul√©: {elapsed/60:.1f}min | Restant: {remaining_time/60:.1f}min")
        
        total_duration = time.time() - start_time
        
        # Sauvegarde du r√©sultat final
        final_result = {
            'success': True,
            'timestamp': time.time(),
            'total_duration_seconds': round(total_duration, 2),
            'total_duration_minutes': round(total_duration / 60, 2),
            'summary': {
                'total_centers': total_centers,
                'total_departments': len(departments),
                'successful_departments': len([d for d in department_stats.values() if d['status'] == 'success']),
                'failed_departments': len([d for d in department_stats.values() if d['status'] != 'success']),
                'average_centers_per_department': round(total_centers / len(departments), 1) if departments else 0,
                'processing_rate_centers_per_second': round(total_centers / total_duration, 1) if total_duration > 0 else 0
            },
            'department_statistics': department_stats,
            'errors': errors,
            'data': {
                'total_centers': total_centers,
                'centers': all_centers
            }
        }
        
        # Sauvegarder le r√©sultat complet
        final_file = os.path.join(output_dir, "all_french_centers.json")
        with open(final_file, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, indent=2, ensure_ascii=False)
        
        print(f"\n=== R√âCUP√âRATION TERMIN√âE ===")
        print(f"‚úÖ {total_centers} centres r√©cup√©r√©s")
        print(f"‚è±Ô∏è  Dur√©e totale: {total_duration/60:.1f} minutes")
        print(f"üìÅ Fichier final: {final_file}")
        print(f"üìÇ Fichiers par d√©partement: {output_dir}/dept_*.json")
        
        if errors:
            print(f"‚ö†Ô∏è  {len(errors)} erreurs rencontr√©es:")
            for error in errors[:5]:  # Afficher les 5 premi√®res erreurs
                print(f"   - {error}")
            if len(errors) > 5:
                print(f"   ... et {len(errors)-5} autres erreurs")
        
        return final_result

def main():
    scraper = UTACScraper()
    
    # Test de recherche par num√©ro d'agr√©ment
    print("=== Test de recherche par num√©ro d'agr√©ment ===")
    
    # D'abord, analyser la structure du formulaire
    print("\n1. Analyse de la structure du formulaire:")
    result = scraper.scrape_ct_search_page()
    
    if result and result['forms']:
        form = result['forms'][0]
        print(f"Formulaire trouv√©: {form['id']}")
        print("Champs disponibles:")
        for inp in form['inputs']:
            if inp['name'] and inp['type'] in ['text', 'select']:
                print(f"  - {inp['name']} ({inp['type']})")
    
    # Test avec un num√©ro d'agr√©ment d'exemple (vous devrez fournir un vrai num√©ro)
    print("\n2. Pour tester la recherche, utilisez:")
    print("scraper.search_by_agreement_number('NUMERO_AGREMENT')")
    print("\nExemple d'utilisation:")
    print("python3 -c \"")
    print("from utac_scraper import UTACScraper")
    print("scraper = UTACScraper()")
    print("result = scraper.search_by_agreement_number('12345')")
    print("print(result)")
    print("\"")

def search_agreement(agreement_number):
    """Fonction utilitaire pour rechercher un num√©ro d'agr√©ment"""
    scraper = UTACScraper()
    
    print(f"=== Recherche du num√©ro d'agr√©ment: {agreement_number} ===")
    result = scraper.search_by_agreement_number(agreement_number)
    
    if result:
        if 'error' in result:
            print(f"Erreur: {result['error']}")
            if 'debug' in result:
                print("Mode debug activ√©")
        else:
            print("=== Informations trouv√©es ===")
            print(f"Num√©ro d'agr√©ment: {result.get('agreement_number', 'N/A')}")
            print(f"Raison sociale: {result.get('raison_sociale', 'N/A')}")
            print(f"Enseigne: {result.get('enseigne', 'N/A')}")
            print(f"Adresse: {result.get('adresse', 'N/A')}")
            print(f"Ville: {result.get('ville', 'N/A')}")
            print(f"T√©l√©phone: {result.get('telephone', 'N/A')}")
            print(f"Option: {result.get('option', 'N/A')}")
            print(f"Site internet: {result.get('site_internet', 'N/A')}")
            print(f"URL d√©tail: {result.get('url', 'N/A')}")
    else:
        print("Aucun r√©sultat trouv√©")

if __name__ == "__main__":
    main()